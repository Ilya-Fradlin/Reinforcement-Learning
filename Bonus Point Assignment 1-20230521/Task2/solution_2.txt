Task 2 - Value Iteration

Answers:


6) 	Rounds of value iteration for start state to become non-zero: 12
    Why?
    Since we initialize all the non-terminal states to have a value of 0, in order for the initial state
    to differ from zero, we have to "propagate" the 'exit' reward along the path between the initial state
    and the final exit state. The shortest path between them, which also dictates the minimal number of iterations
    required is indeed 12.

7) 	Which parameter to change: discount
	Value of the changed parameter: 0

8)	Parameter values producing optimal policy types:
	    a) -n 0 -d 0.1
	    b) -n 0.1 -d 0.1
	    c) -n 0 -d 1
	    d) -n 0.3 -d 1
	    e) not possible

9) 	Pros:
		- Policy iteration guarantees convergence to the optimal policy and value function, regardless of the initial policy.
		- This can result in a better, more precise policy, since it takes into account more 'information'.

    Cons:
        - As stated in the lecture, Policy iteration can be computationally expensive, especially for large MDPs.
          The process of policy evaluation and policy improvement requires multiple iterations until convergence,
          which can be time-consuming for complex problems (due to the computation required each time to evaluate all the states). However, if we look at value iteration, we do not need to iterate
          between the evaluate and improve functions, reaching the solution much faster (especially in large state domains)
        - In rare cases, policy iteration can also get stuck in a local optima, where the policy improvement step fails to find a better policy.