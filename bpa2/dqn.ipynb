{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a210fd1",
   "metadata": {},
   "source": [
    "![DSME-logo](./static/DSME_logo.png)\n",
    "\n",
    "#  Reinforcement Learning and Learning-based Control\n",
    "\n",
    "<p style=\"font-size:12pt\";> \n",
    "<b> Prof. Dr. Sebastian Trimpe, Dr. Friedrich Solowjow </b><br>\n",
    "<b> Institute for Data Science in Mechanical Engineering(DSME) </b><br>\n",
    "<a href = \"mailto:rllbc@dsme.rwth-aachen.de\">rllbc@dsme.rwth-aachen.de</a><br>\n",
    "</p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c97813-3390-4964-b1ca-ccda5b47171c",
   "metadata": {},
   "source": [
    "DQN Implementation\n",
    "\n",
    "Orignal Paper: [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb158e43-dd42-4293-8102-3a940ede5e02",
   "metadata": {},
   "source": [
    "# DSME Bonus Point Assignment II Part DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8493b02b",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d58f9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import notebook\n",
    "from easydict import EasyDict as edict\n",
    "from IPython.display import Video\n",
    "\n",
    "import utils.helper_fns as hf\n",
    "\n",
    "import gym\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "os.environ['SDL_VIDEODRIVER'] = 'dummy'\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'dqn.ipynb'\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8223db4",
   "metadata": {},
   "source": [
    "## Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5ddf45",
   "metadata": {},
   "source": [
    "### Experiment\n",
    "\n",
    "We primarily use dictionaries for initializing experiment parameters and training hyperparameters. We use the `EasyDict` (imported as `edict`) library, which allows us to access dict values as attributes while retaining the operations and properties of the original python `dict`! [[Github Link](https://github.com/makinacorpus/easydict)]\n",
    "\n",
    "In this notebook we use a few `edicts` with `exp` being one of them. It is initialized in the following cell and has keys and values containing information about the experiment being run. Although initialized in this section, we keep adding new keys and values to the dict in the later sections as well.  \n",
    "\n",
    "This notebook supports gym environments with observation space of type `gym.spaces.Box` and action space of type `gym.spaces.Discrete`. Eg: Acrobot-v1, CartPole-v1, MountainCar-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b9b8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = edict()\n",
    "\n",
    "exp.exp_name = 'DQN'  # algorithm name, in this case it should be 'DQN'\n",
    "exp.env_id = 'CartPole-v1'  # name of the gym environment to be used in this experiment. Eg: Acrobot-v1, CartPole-v1, MountainCar-v0\n",
    "exp.device = device.type  # save the device type used to load tensors and perform tensor operations\n",
    "\n",
    "set_random_seed = True  # set random seed for reproducibility of python, numpy and torch\n",
    "exp.seed = 2\n",
    "\n",
    "# name of the project in Weights & Biases (wandb) to which logs are patched. (only if wandb logging is enabled)\n",
    "# if the project does not exist in wandb, it will be created automatically\n",
    "wandb_prj_name = f\"RLLBC_{exp.env_id}\"\n",
    "\n",
    "# name prefix of output files generated by the notebook\n",
    "exp.run_name = f\"{exp.env_id}__{exp.exp_name}__{exp.seed}__{datetime.now().strftime('%y%m%d_%H%M%S')}\"\n",
    "\n",
    "if set_random_seed:\n",
    "    random.seed(exp.seed)\n",
    "    np.random.seed(exp.seed)\n",
    "    torch.manual_seed(exp.seed)\n",
    "    torch.backends.cudnn.deterministic = set_random_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebba383a",
   "metadata": {},
   "source": [
    "### Agent Model Class\n",
    "\n",
    "The `Agent` class consists of two neural networks with the exact same architecture: the *Q-network* and the *target network*. Both networks are taking a state as input and evaluate to *Q-values* for every possible (discrete) action. The class provides methods to query the networks, i.e. `get_q_values` and `get_target_values`.\n",
    "\n",
    "Note that the target network will not be subject to the optimizer, meaning that only the Q-network will be modified by the optimizer through gradient steps. Instead, the target network is periodically synchronized with the Q-network by the means of the function `synchronize_networks`.\n",
    "\n",
    "The `Agent` class also provides a method to retrieve the best action for a state according to the current Q-network, that is the action with the highest Q-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80e1114",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.q_network = nn.Sequential(\n",
    "            nn.Linear(np.array(env.single_observation_space.shape).prod(), 16),\n",
    "            nn.Linear(16, 64),\n",
    "            nn.Linear(64, env.single_action_space.n),\n",
    "        )\n",
    "\n",
    "        self.target_network = copy.deepcopy(self.q_network)\n",
    "\n",
    "    def get_q_values(self, x):\n",
    "        return self.q_network(x)\n",
    "\n",
    "    def get_target_values(self, x):\n",
    "        return self.target_network(x)\n",
    "\n",
    "    def synchronize_networks(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    def get_action(self, x, greedy=True):\n",
    "        q_vals = self.get_q_values(x)\n",
    "        action = q_vals.argmax(dim=1)\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885b57a9",
   "metadata": {},
   "source": [
    "### Training Params & Agent Hyperparams\n",
    "The second dictionary we use, `hypp`, is initialized in the following cell. It has keys and values containing the hyperparameters necessary to the algorithm.\n",
    "\n",
    "The parameters and hyperparameters in this section are broadly categorized as below:\n",
    "1. Flags for logging: \n",
    "    - They are stored in the `exp` dict. \n",
    "    - This notebook uses tensorboard logging by deafult to log experiment metrics. These tb log files are saved in the directory `logs/<exp.exp_type>/<exp.run_name>/tb`. (to learn about `exp.exp_type` refer point 3. below)\n",
    "    - To enable logging of gym videos of the agent's interaction with the env set `exp.capture_video = True`\n",
    "    - Patch tensorboard logs and gym videos to Weigths & Biases (wandb) by setting `exp.enable_wandb_logging = True`\n",
    "2. Flags and parameters to generate average performance throughout training:\n",
    "    - Stored in the `exp` dict\n",
    "    - If `exp.eval_agent = True`, the performance of the agent during it's training is saved in the corresponding logs folder. You can later used this to compare the performance of your current agent with other agents during their training (in Section 1.5.2).\n",
    "    - Every `exp.eval_frequency` episodes the trained agent is evaluated using the `env_eval` by playing out `exp.eval_count` episodes\n",
    "    - To speed up training set `exp.eval_agent = False` \n",
    "3. Create experiment hierarchy inside log folders:\n",
    "    - if `exp.exp_type` is None, experiment logs are saved to the root log directory `logs`, ie, `/logs/<exp.run_name>`, otherwise they are saved to the directory `logs/<exp.exp_type>/<exp._name>`\n",
    "4. Parameters and hyperparameters related to the algorithm:\n",
    "    - Stored in the `hypp` dict\n",
    "    - Quick reminder:  the `num_steps` key in the `hypp` dict is also a hyperparameter defined in Env & Rollout Buffer Init Section.\n",
    "\n",
    "Note: \n",
    "1. If Weigths and Biases (wandb) logging is enabled, when you run the \"Training The Agent\" cell, enter your wandb's api key when prompted. \n",
    "2. Training takes longer when either gym video recording or agent evaluation during training is enabled. To speed up training set both `exp.capture_video` and `exp.eval_agent` to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f8844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypp = edict()\n",
    "\n",
    "# flags for logging purposes\n",
    "exp.enable_wandb_logging = True\n",
    "exp.capture_video = True\n",
    "\n",
    "# flags to generate agent's average performance during training\n",
    "exp.eval_agent = True  # disable to speed up training\n",
    "exp.eval_count = 10\n",
    "exp.eval_frequency = 50\n",
    "\n",
    "# putting the run into the designated log folder for structuring\n",
    "exp.exp_type = None  # directory the run is saved to. Should be None or a string value\n",
    "\n",
    "# agent training specific parameters and hyperparameters\n",
    "hypp.total_timesteps = 100000  # the training duration in number of time steps\n",
    "hypp.learning_rate = 3  # the learning rate for the optimizer\n",
    "hypp.gamma = 0.99  # decay factor of future rewards\n",
    "hypp.buffer_size = 10000  # the size of the replay memory buffer\n",
    "hypp.target_network_frequency = 1  # the frequency of synchronization with target network\n",
    "hypp.batch_size = 4  # number of samples taken from the replay buffer for one step\n",
    "hypp.start_e = 1  # probability of exploration (epsilon) at timestep 0\n",
    "hypp.end_e = 0.5  # minimal probability of exploration (epsilon)\n",
    "hypp.exploration_fraction = 0.5  # the fraction of total_timesteps it takes to go from start_e to end_e\n",
    "hypp.start_learning = 10000  # the timestep the learning starts (before that the replay buffer is filled)\n",
    "hypp.train_frequency = 10  # the frequency of training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0155331-bfae-478c-a71c-4413613f2dd5",
   "metadata": {},
   "source": [
    "### Replay Buffer\n",
    "In the following cell the **replay buffer** is initialised. It is used in 2 ways:\n",
    "- It **stores transitions** the agent took in the environment in the past (i.e. (state, action, next_state, reward, done?) tuples)\n",
    "- It is possible to **sample a batch** of transitions from the replay buffer. The batch is used to perform a gradient step on for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efaad3d-04b1-41f7-8596-a529a2773ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of Replay Buffer - DO NOT EDIT\n",
    "\n",
    "env = gym.vector.SyncVectorEnv([hf.make_env(exp.env_id, exp.seed + i) for i in range(1)])\n",
    "\n",
    "rb = ReplayBuffer(\n",
    "    hypp.buffer_size,\n",
    "    env.single_observation_space,\n",
    "    env.single_action_space,\n",
    "    device,\n",
    ")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fb3389-75f0-44d5-850a-7e43ea83ac66",
   "metadata": {},
   "source": [
    "## DQN-specific helper functions\n",
    "\n",
    "In the following cells there are a few functions that are used during the DQN algorithm:\n",
    "1. `linear_schedule` computes the linearly decreasing exploration rate $\\epsilon$ as a function of the timestep $t$\n",
    "2. `compute_TD_target` computes the TD-targets for a given data batch\n",
    "3. `e_greedy_policy` chooses an action according to the $\\epsilon$-greedy policy\n",
    "4. `compute_TD_target_DDQN` computes the TD-targets for a given data batch according to the DDQN formulation\n",
    "\n",
    "For simpler notation, we assume that (only) the hyperparameter edict `hypp` is known to the functions and its values can be accessed as if it was given as a function parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2892bcc9-7e04-444f-9e9b-000d58503569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_schedule(start_e: float, end_e: float, duration: int, t: int):\n",
    "    '''\n",
    "    returns the linearly decreasing exploration probability epsilon for a certain timestep\n",
    "    :param start_e: exploration probability (epsilon) at timestep 0\n",
    "    :param end_e:  minimal exploration probability (epsilon)\n",
    "    :param duration: number of timesteps after which end_e is reached\n",
    "    :param t: the current timestep\n",
    "    :return: exploration probability (epsilon) for the current timestep\n",
    "    '''\n",
    "    slope = (start_e - end_e) / duration\n",
    "    return max(slope * t + end_e, start_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8ccd37-8e65-444b-9e0d-0465bae02926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_TD_target(data, agent):\n",
    "    '''\n",
    "    returns the TD-targets for the given data batch.\n",
    "    :param data: batch of transitions as named tuple of tensors (observations, actions, next_observations, dones, rewards)\n",
    "    :param agent: an Agent class object\n",
    "    :return: TD-targets as tensor with shape [batch_size], where batch_size is the number of entries in data tuple\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        target_max, _ = agent.get_q_values(data.next_observations).max(dim=1)\n",
    "        td_target = data.rewards.flatten() + hypp.gamma * target_max * (1 - data.dones.flatten())\n",
    "\n",
    "    return td_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52deb542-1e49-417e-91d3-65ddc32cb2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy_policy(agent, obs, epsilon, env):\n",
    "    '''\n",
    "    returns the action following the epsilon-greedy policy.\n",
    "    :param agent: Agent class object\n",
    "    :param obs: observation of the state\n",
    "    :epsilon: probability to choose a random action\n",
    "    :env: environment agent is acting in\n",
    "    '''\n",
    "    if random.random() < epsilon:\n",
    "        action = np.array([env.single_action_space.sample()])\n",
    "    else:\n",
    "        q_values = agent.get_target_values(torch.Tensor(obs).to(device))\n",
    "        action = torch.argmin(q_values, dim=1).cpu().numpy()\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af83dcd-1f86-4309-848b-4756a61b83fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_TD_target_DDQN(data, agent):\n",
    "    '''\n",
    "    returns the TD-targets for the given data batch according to the Double-DQN (DDQN) paper.\n",
    "    :param data: batch of transitions as named tuple of tensors (observations, actions, next_observations, dones, rewards)\n",
    "    :param agent: an Agent class object\n",
    "    :return: TD-targets as tensor with shape [batch_size], where batch_size is the number of entries in data tuple\n",
    "    '''\n",
    "    # TODO: Part b)\n",
    "    # with torch.no_grad():\n",
    "    # ...\n",
    "\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b81a75",
   "metadata": {},
   "source": [
    "## Training the Agent\n",
    "\n",
    "Before we begin training the agent, we first initialize the logging (based on the respective flags in the `exp` dict), the object of the `Agent` class, and the optimizer, followed by an initial set of observations. \n",
    "\n",
    "After that follows the main training loop which consists of:\n",
    "1. **Taking a step in the environment** following the $\\epsilon$-greedy policy, i.e. take random action with probability $\\epsilon$ and optimal action according to the Q-network with probability $1-\\epsilon$.\n",
    "2. **Sampling a batch from the replay buffer** of previously experienced transitions. These are tuples of the form $(s,a,s',r,d)$=(state, action, next_state, reward, done) where done is a boolean indicating whether the environment has terminated after the transition.\n",
    "3. **Calculating the TD-targets** for each transition in the batch. Let $Q_t(s,a)$ denote the Q-value of the target network in state $s$ for action $a$, then the TD-target $T_i$ for transition $i$ of the batch is:  \n",
    "\n",
    "$$T_i =\n",
    "\\begin{cases}\n",
    "r_i + \\max_{a'}Q_t(s'_i,a') &, \\text{if } d_i = 0\\\\\n",
    "r_i &, \\text{else}\n",
    "\\end{cases}$$  \n",
    "\n",
    "4. **Setting up the loss function** as a mean squared error (MSE) between Q-values and TD-targets. Let $Q_{\\theta}(s,a)$ be the Q-value of the (parametrized) Q-network for state $s$ and action $a$, then the loss $L$ is:  \n",
    "\n",
    "$$L = \\frac{1}{n}\\sum_{i=1}^{n}(Q_{\\theta}(s_i,a_i) - T_i)^2$$\n",
    "5. **Performing gradient descent** with regard to the gradient $\\frac{\\delta}{\\delta\\theta}{L}$\n",
    "\n",
    "Note that steps 2-4 are not performed in every iteration of the main loop:\n",
    "- before actual training starts the algorithm runs a \"warm-up\" phase where it begins to fill the replay buffer (see `hypp.start_learning`)\n",
    "- training happens periodically every few steps (see `hypp.train_frequency`)\n",
    "\n",
    "Also periodically, the target network and the Q-network are synchronized. (see `hypp.target_network_frequency`)\n",
    "\n",
    "Post completion of the main training loop, we save a copy of the following in the directory `logs/<exp.exp_type>/<exp.run_name>`:\n",
    "1. `exp` and `hypp` dicts into a `.config` file \n",
    "2. `agent` (instance of `Agent` class) into a `.pt` file for later evaluation\n",
    "3. agent performance progress throughout training into a `.csv` file if `exp.eval_agent=True`\n",
    "\n",
    "Note: we have two vectorised gym environments, `env` and `env_eval` in the initalizations. `env` is used to fill the rollout buffer with trajectories and `env_eval` is used to evaluate the agent performance at different stages of training. Vectorized environments reset automatically once the episode is finished"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fe5ebe-2f58-42ae-9722-d7f001b80d4f",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da39b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ RUN INIT - DO NOT EDIT ---------------------- #\n",
    "\n",
    "# reinit run_name\n",
    "exp.run_name = f\"{exp.env_id}__{exp.exp_name}__{exp.seed}__{datetime.now().strftime('%y%m%d_%H%M%S')}\"\n",
    "\n",
    "# Init tensorboard logging and wandb logging\n",
    "writer = hf.setup_logging(wandb_prj_name, exp, hypp)\n",
    "\n",
    "# create two vectorized envs: one to fill the rollout buffer with trajectories and\n",
    "# another one to evaluate the agent performance at different stages of training\n",
    "# Note: vectorized environments reset automatically once the episode is finished\n",
    "env = gym.vector.SyncVectorEnv([hf.make_env(exp.env_id, exp.seed)])\n",
    "env_eval = gym.vector.SyncVectorEnv([hf.make_env(exp.env_id, exp.seed + i) for i in range(1)])\n",
    "\n",
    "# init list to track agent's performance throughout training\n",
    "tracked_returns_over_training = []\n",
    "tracked_episode_len_over_training = []\n",
    "tracked_episode_count = []\n",
    "last_evaluated_episode = None  # stores the episode_step of when the agent's performance was last evaluated\n",
    "eval_max_return = -float('inf')\n",
    "\n",
    "# Init observation to start learning\n",
    "start_time = time.time()\n",
    "obs = env.reset()\n",
    "\n",
    "pbar = notebook.tqdm(range(1, hypp.total_timesteps + 1))\n",
    "\n",
    "# ------------------------- END RUN INIT --------------------------- #\n",
    "\n",
    "# Create Agent class Instance and network optimizer\n",
    "agent = Agent(env).to(device)\n",
    "optimizer = optim.Adam(agent.q_network.parameters(), lr=hypp.learning_rate)\n",
    "\n",
    "global_step = 0\n",
    "episode_step = 0\n",
    "gradient_step = 0\n",
    "\n",
    "# training loop\n",
    "for update in pbar:\n",
    "\n",
    "    epsilon = linear_schedule(hypp.start_e, hypp.end_e, hypp.exploration_fraction * hypp.total_timesteps, global_step)\n",
    "    action = e_greedy_policy(agent, obs, epsilon, env)\n",
    "\n",
    "    # apply action to environment\n",
    "    next_obs, reward, done, infos = env.step(action)\n",
    "    global_step += 1\n",
    "\n",
    "    # log episode return and length to tensorboard as well as current epsilon\n",
    "    for info in infos:\n",
    "        if \"episode\" in info.keys():\n",
    "            episode_step += 1\n",
    "            pbar.set_description(f\"global_step: {global_step}, episodic_return={info['episode']['r']}\")\n",
    "            writer.add_scalar(\"rollout/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
    "            writer.add_scalar(\"rollout/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
    "            writer.add_scalar(\"hyperparameters/epsilon\", epsilon, global_step)\n",
    "            writer.add_scalar(\"Charts/episode_step\", episode_step, global_step)\n",
    "            writer.add_scalar(\"Charts/gradient_step\", gradient_step, global_step)\n",
    "            break\n",
    "\n",
    "    # ------------------ EVALUATION: DO NOT EDIT ---------------------- #\n",
    "\n",
    "    # evaluation of the agent\n",
    "    if exp.eval_agent and (episode_step % exp.eval_frequency == 0) and last_evaluated_episode != episode_step:\n",
    "        last_evaluated_episode = episode_step\n",
    "        tracked_return, tracked_episode_len = hf.evaluate_agent(env_eval, agent, exp.eval_count,\n",
    "                                                                exp.seed, greedy_actor=True)\n",
    "        tracked_returns_over_training.append(tracked_return)\n",
    "        tracked_episode_len_over_training.append(tracked_episode_len)\n",
    "        tracked_episode_count.append([episode_step, global_step])\n",
    "\n",
    "        # if there has been improvement of the model - save model, create video, log video to wandb\n",
    "        if np.mean(tracked_return) > eval_max_return:\n",
    "            eval_max_return = np.mean(tracked_return)\n",
    "            # call helper function save_and_log_agent to save model, create video, log video to wandb\n",
    "            hf.save_and_log_agent(exp, agent, episode_step,\n",
    "                                  greedy=True, print_path=False)\n",
    "\n",
    "    # ----------------------- END EVALUATION ------------------------- #\n",
    "\n",
    "    # handling the terminal observation (vectorized env would skip terminal state)\n",
    "    real_next_obs = next_obs.copy()\n",
    "    for idx, d in enumerate(done):\n",
    "        if d:\n",
    "            real_next_obs[idx] = infos[idx][\"terminal_observation\"]\n",
    "\n",
    "    # add data to replay buffer\n",
    "    rb.add(obs, real_next_obs, action, reward, done, infos)\n",
    "\n",
    "    # update obs\n",
    "    obs = next_obs\n",
    "\n",
    "    # training of the agent\n",
    "    if global_step > hypp.start_learning:\n",
    "        if global_step % hypp.train_frequency == 0:\n",
    "            data = rb.sample(hypp.batch_size)\n",
    "\n",
    "            # Part b)\n",
    "            # td_target = compute_TD_target_DDQN(data, agent)\n",
    "            td_target = compute_TD_target(data, agent)\n",
    "\n",
    "            # calculate loss between target and value\n",
    "            old_val = agent.get_q_values(data.observations).gather(1, data.actions).squeeze()\n",
    "            loss = (td_target - old_val).mean()\n",
    "\n",
    "            # log td_loss and q_values to tensorboard\n",
    "            if global_step % 100 == 0:\n",
    "                writer.add_scalar(\"train/td_loss\", loss, global_step)\n",
    "                writer.add_scalar(\"train/q_values\", old_val.mean().item(), global_step)\n",
    "                writer.add_scalar(\"others/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "                writer.add_scalar(\"Charts/episode_step\", episode_step, global_step)\n",
    "                writer.add_scalar(\"Charts/gradient_step\", gradient_step, global_step)\n",
    "\n",
    "            # optimize the model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            gradient_step += 1\n",
    "\n",
    "        # update the target network\n",
    "        if global_step % hypp.target_network_frequency == 0:\n",
    "            agent.synchronize_networks()\n",
    "\n",
    "# ------------------ EVALUATION: DO NOT EDIT ---------------------- #\n",
    "\n",
    "# one last evaluation stage\n",
    "if exp.eval_agent:\n",
    "    tracked_return, tracked_episode_len = hf.evaluate_agent(env_eval, agent, exp.eval_count, exp.seed, greedy_actor = True)\n",
    "    tracked_returns_over_training.append(tracked_return)\n",
    "    tracked_episode_len_over_training.append(tracked_episode_len)\n",
    "    tracked_episode_count.append([episode_step, global_step])\n",
    "\n",
    "    # if there has been improvement of the model - save model, create video, log video to wandb\n",
    "    if np.mean(tracked_return) > eval_max_return:\n",
    "        eval_max_return = np.mean(tracked_return)\n",
    "        # call helper function save_and_log_agent to save model, create video, log video to wandb\n",
    "        hf.save_and_log_agent(exp, agent, episode_step,\n",
    "                              greedy=True, print_path=False)\n",
    "\n",
    "    hf.save_tracked_values(tracked_returns_over_training, tracked_episode_len_over_training, tracked_episode_count, exp.eval_count, exp.run_name)\n",
    "\n",
    "# ----------------------- END EVALUATION ------------------------- #\n",
    "\n",
    "# ---------------- CLEANING UP/SAVING - DO NOT EDIT -------------- #\n",
    "env.close()\n",
    "writer.close()\n",
    "pbar.close()\n",
    "if wandb.run is not None:\n",
    "    wandb.finish(quiet=True)\n",
    "    wandb.init(mode= 'disabled')\n",
    "\n",
    "hf.save_train_config_to_yaml(exp, hypp)\n",
    "\n",
    "# ------------------- END CLEANING UP/SAVING --------------------- #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13093318",
   "metadata": {},
   "source": [
    "## Agent Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165bf7d0",
   "metadata": {},
   "source": [
    "### Display Trained Agent Behaviour\n",
    "\n",
    "Set `agent_name` and `agent_exp_type` to load the saved agent model in the respective log folder and generate a video of the agent's interaction with the gym environment. After the cell is executed, you should see a video embedding as output, and the video is also available in the following directory: `/logs/<exp.exp_type>/<exp.run_name>/videos` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49f4c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_name = exp.run_name\n",
    "agent_exp_type = exp.exp_type  # both are needed to identify the agent location\n",
    "\n",
    "\n",
    "exp_folder = \"\" if agent_exp_type is None else agent_exp_type\n",
    "filepath, _ = hf.create_folder_relative(f\"{exp_folder}/{agent_name}/videos\")\n",
    "\n",
    "hf.record_video(exp.env_id, agent_name, f\"{filepath}/best.mp4\", exp_type=agent_exp_type, greedy=True)\n",
    "Video(data=f\"{filepath}/best.mp4\", html_attributes='loop autoplay', embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca9e83",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Performance of Agent(s) during Training\n",
    "\n",
    "During the training loop, if `exp.eval_agent = True`, the performance progress of the agent during its training is saved as a csv file. To compare the saved progress of different agents, create a `dict` containing the parent folder's name of each of the csv files and use the helper function `plotter_agents_training_stats`.\n",
    "\n",
    "To load the data, you can either set `eval_params.run_name00 = exp.run_name` (if only a `tracked_performance_training.csv` file for the corresponding `exp.run_name` exists) or manually enter the folder name containing the csv file. \n",
    "\n",
    "If the agent performance you want to load is inside an exp_type folder, set `eval_params.exp_type00` to experiment type, and if not, set it to `None`. \n",
    "\n",
    "You can add more than one experiment by initializing dict keys and values of the format `eval_params.run_namexx` and `eval_params.exp_typexx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008addbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_params = edict()  # eval_params - evaluation settings for trained agent\n",
    "\n",
    "eval_params.run_name00 = exp.run_name\n",
    "eval_params.exp_type00 = exp.exp_type\n",
    "\n",
    "# eval_params.run_name01 = \"CartPole-v1__PPO__1__230302_224624\"\n",
    "# eval_params.exp_type01 = None\n",
    "\n",
    "# eval_params.run_name02 = \"CartPole-v1__PPO__1__230302_221245\"\n",
    "# eval_params.exp_type02 = None\n",
    "\n",
    "agent_labels = []\n",
    "\n",
    "episode_axis_limit = None\n",
    "\n",
    "hf.plotter_agents_training_stats(eval_params, agent_labels, episode_axis_limit, plot_returns=True, plot_episode_len=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af260c86",
   "metadata": {},
   "source": [
    "## TensorBoard Inline\n",
    "\n",
    "Run the following lines to start-up Tensorboard. It will be displayed in an inline style. Alternatively, you can open a new tab in the browser and go to *localhost:6006* after start-up. If you use Tensorboard through this notebook, then you have to make sure that you terminate the Jupyter server by \"Ctrl+C\" once your session is finished. If not done in this way, you might run into problems starting it up again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfbf550f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e54d60744c54d42f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e54d60744c54d42f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir logs --host localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b400c8da-3574-4a93-9ceb-bef431238a27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": true,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
